PRAGMA foreign_keys=OFF;
BEGIN TRANSACTION;
CREATE TABLE IF NOT EXISTS "django_migrations" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "app" varchar(255) NOT NULL, "name" varchar(255) NOT NULL, "applied" datetime NOT NULL);
INSERT INTO django_migrations VALUES(1,'contenttypes','0001_initial','2023-12-06 08:41:56.738558');
INSERT INTO django_migrations VALUES(2,'auth','0001_initial','2023-12-06 08:41:56.747528');
INSERT INTO django_migrations VALUES(3,'admin','0001_initial','2023-12-06 08:41:56.753508');
INSERT INTO django_migrations VALUES(4,'admin','0002_logentry_remove_auto_add','2023-12-06 08:41:56.759488');
INSERT INTO django_migrations VALUES(5,'admin','0003_logentry_add_action_flag_choices','2023-12-06 08:41:56.764285');
INSERT INTO django_migrations VALUES(6,'contenttypes','0002_remove_content_type_name','2023-12-06 08:41:56.773255');
INSERT INTO django_migrations VALUES(7,'auth','0002_alter_permission_name_max_length','2023-12-06 08:41:56.779235');
INSERT INTO django_migrations VALUES(8,'auth','0003_alter_user_email_max_length','2023-12-06 08:41:56.784219');
INSERT INTO django_migrations VALUES(9,'auth','0004_alter_user_username_opts','2023-12-06 08:41:56.788757');
INSERT INTO django_migrations VALUES(10,'auth','0005_alter_user_last_login_null','2023-12-06 08:41:56.795734');
INSERT INTO django_migrations VALUES(11,'auth','0006_require_contenttypes_0002','2023-12-06 08:41:56.797727');
INSERT INTO django_migrations VALUES(12,'auth','0007_alter_validators_add_error_messages','2023-12-06 08:41:56.802711');
INSERT INTO django_migrations VALUES(13,'auth','0008_alter_user_username_max_length','2023-12-06 08:41:56.807693');
INSERT INTO django_migrations VALUES(14,'auth','0009_alter_user_last_name_max_length','2023-12-06 08:41:56.813229');
INSERT INTO django_migrations VALUES(15,'auth','0010_alter_group_name_max_length','2023-12-06 08:41:56.818213');
INSERT INTO django_migrations VALUES(16,'auth','0011_update_proxy_permissions','2023-12-06 08:41:56.822200');
INSERT INTO django_migrations VALUES(17,'auth','0012_alter_user_first_name_max_length','2023-12-06 08:41:56.827183');
INSERT INTO django_migrations VALUES(18,'homesite','0001_initial','2023-12-06 08:41:56.832166');
INSERT INTO django_migrations VALUES(19,'sessions','0001_initial','2023-12-06 08:41:56.836153');
INSERT INTO django_migrations VALUES(20,'homesite','0002_delete_emailtemplate','2023-12-06 09:56:19.925523');
INSERT INTO django_migrations VALUES(21,'homesite','0003_subscribetonewsletter_delete_subscriber','2023-12-06 11:16:37.134875');
INSERT INTO django_migrations VALUES(22,'homesite','0004_contact_alter_subscribetonewsletter_email','2023-12-07 13:28:52.268783');
INSERT INTO django_migrations VALUES(23,'homesite','0005_portfolio','2023-12-27 06:22:45.581361');
INSERT INTO django_migrations VALUES(24,'homesite','0006_portfolio_category_alter_portfolio_body','2023-12-27 07:31:46.674000');
INSERT INTO django_migrations VALUES(25,'homesite','0007_remove_portfolio_category','2023-12-27 07:59:04.809521');
INSERT INTO django_migrations VALUES(26,'homesite','0008_portfolio_category','2023-12-27 07:59:04.812615');
INSERT INTO django_migrations VALUES(27,'homesite','0009_portfolio_photo_alter_portfolio_category','2023-12-27 09:13:24.314437');
INSERT INTO django_migrations VALUES(28,'homesite','0010_portfolio_insight','2023-12-27 10:28:49.207516');
INSERT INTO django_migrations VALUES(29,'homesite','0011_alter_portfolio_category','2023-12-27 11:43:58.274290');
INSERT INTO django_migrations VALUES(30,'homesite','0012_alter_portfolio_body_alter_portfolio_category','2023-12-27 13:29:38.955577');
INSERT INTO django_migrations VALUES(31,'homesite','0013_alter_portfolio_category','2023-12-30 10:43:01.286628');
INSERT INTO django_migrations VALUES(32,'homesite','0014_alter_portfolio_category','2023-12-30 11:14:39.580689');
INSERT INTO django_migrations VALUES(33,'homesite','0015_blogposts','2023-12-30 13:30:43.728217');
INSERT INTO django_migrations VALUES(34,'homesite','0016_blogposts_subscribed_at','2023-12-30 13:53:42.996341');
INSERT INTO django_migrations VALUES(35,'homesite','0017_alter_blogposts_options','2024-01-24 12:54:09.114839');
CREATE TABLE IF NOT EXISTS "auth_group_permissions" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "group_id" integer NOT NULL REFERENCES "auth_group" ("id") DEFERRABLE INITIALLY DEFERRED, "permission_id" integer NOT NULL REFERENCES "auth_permission" ("id") DEFERRABLE INITIALLY DEFERRED);
CREATE TABLE IF NOT EXISTS "auth_user_groups" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "user_id" integer NOT NULL REFERENCES "auth_user" ("id") DEFERRABLE INITIALLY DEFERRED, "group_id" integer NOT NULL REFERENCES "auth_group" ("id") DEFERRABLE INITIALLY DEFERRED);
CREATE TABLE IF NOT EXISTS "auth_user_user_permissions" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "user_id" integer NOT NULL REFERENCES "auth_user" ("id") DEFERRABLE INITIALLY DEFERRED, "permission_id" integer NOT NULL REFERENCES "auth_permission" ("id") DEFERRABLE INITIALLY DEFERRED);
CREATE TABLE IF NOT EXISTS "django_admin_log" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "object_id" text NULL, "object_repr" varchar(200) NOT NULL, "action_flag" smallint unsigned NOT NULL CHECK ("action_flag" >= 0), "change_message" text NOT NULL, "content_type_id" integer NULL REFERENCES "django_content_type" ("id") DEFERRABLE INITIALLY DEFERRED, "user_id" integer NOT NULL REFERENCES "auth_user" ("id") DEFERRABLE INITIALLY DEFERRED, "action_time" datetime NOT NULL);
INSERT INTO django_admin_log VALUES(1,'1','mina@gmail.com',1,'[{"added": {}}]',9,1,'2023-12-06 11:17:28.252452');
INSERT INTO django_admin_log VALUES(2,'2','mina2@gmail.com',1,'[{"added": {}}]',9,1,'2023-12-06 12:01:20.032831');
INSERT INTO django_admin_log VALUES(3,'3','mina3@gmail.com',1,'[{"added": {}}]',9,1,'2023-12-06 12:01:32.156597');
INSERT INTO django_admin_log VALUES(4,'1','hello world!',1,'[{"added": {}}]',10,1,'2023-12-07 13:29:59.995004');
INSERT INTO django_admin_log VALUES(5,'1','Test 1',1,'[{"added": {}}]',11,1,'2023-12-27 06:23:36.738166');
INSERT INTO django_admin_log VALUES(6,'2','test 2',1,'[{"added": {}}]',11,1,'2023-12-27 07:05:09.843728');
INSERT INTO django_admin_log VALUES(7,'3','ff',1,'[{"added": {}}]',11,1,'2023-12-27 07:39:07.398700');
INSERT INTO django_admin_log VALUES(8,'3','ff',3,'',11,1,'2023-12-27 07:57:38.771513');
INSERT INTO django_admin_log VALUES(9,'2','test 2',3,'',11,1,'2023-12-27 07:57:38.774546');
INSERT INTO django_admin_log VALUES(10,'1','Test 1',3,'',11,1,'2023-12-27 07:57:38.775541');
INSERT INTO django_admin_log VALUES(11,'1','TEst 1',1,'[{"added": {}}]',11,1,'2023-12-27 07:59:25.222738');
INSERT INTO django_admin_log VALUES(12,'1','TEst 1',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-27 08:08:14.059360');
INSERT INTO django_admin_log VALUES(13,'1','TEst 1',2,'[{"changed": {"fields": ["Category", "Photo"]}}]',11,1,'2023-12-27 09:13:50.730319');
INSERT INTO django_admin_log VALUES(14,'1','TEst 1',2,'[{"changed": {"fields": ["Insight"]}}]',11,1,'2023-12-27 10:29:12.513414');
INSERT INTO django_admin_log VALUES(15,'2','Test 2',1,'[{"added": {}}]',11,1,'2023-12-27 10:30:15.782973');
INSERT INTO django_admin_log VALUES(16,'1','TEst 1',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-27 10:39:34.237331');
INSERT INTO django_admin_log VALUES(17,'1','TEst 1',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-27 10:40:23.861765');
INSERT INTO django_admin_log VALUES(18,'2','Test 2',2,'[]',11,1,'2023-12-27 11:24:09.036551');
INSERT INTO django_admin_log VALUES(19,'1','TEst 1',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:24:13.707169');
INSERT INTO django_admin_log VALUES(20,'1','TEst 1',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:26:44.267912');
INSERT INTO django_admin_log VALUES(21,'1','TEst 1',2,'[]',11,1,'2023-12-27 11:42:35.692072');
INSERT INTO django_admin_log VALUES(22,'2','Test 2',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:42:40.220510');
INSERT INTO django_admin_log VALUES(23,'1','TEst 1',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:43:08.926119');
INSERT INTO django_admin_log VALUES(24,'1','TEst 1',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:44:30.715057');
INSERT INTO django_admin_log VALUES(25,'1','TEst 1',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:44:54.099773');
INSERT INTO django_admin_log VALUES(26,'1','TEst 1',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:44:57.985588');
INSERT INTO django_admin_log VALUES(27,'2','Test 2',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:45:01.436373');
INSERT INTO django_admin_log VALUES(28,'1','TEst 1',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:45:36.330849');
INSERT INTO django_admin_log VALUES(29,'1','TEst 1',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:45:49.181053');
INSERT INTO django_admin_log VALUES(30,'2','Test 2',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-27 11:57:38.643648');
INSERT INTO django_admin_log VALUES(31,'1','TEst 1',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-27 13:36:47.397796');
INSERT INTO django_admin_log VALUES(32,'3','Outsourcing Commercial Warehouses to a Logistics Partner',1,'[{"added": {}}]',11,1,'2023-12-27 14:41:09.299824');
INSERT INTO django_admin_log VALUES(33,'3','Outsourcing Commercial Warehouses to a Logistics Partner',2,'[{"changed": {"fields": ["Photo"]}}]',11,1,'2023-12-27 15:16:01.623373');
INSERT INTO django_admin_log VALUES(34,'3','Outsourcing Commercial Warehouses to a Logistics Partner',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-27 15:33:17.429322');
INSERT INTO django_admin_log VALUES(35,'3','Outsourcing Commercial Warehouses to a Logistics Partner',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-27 15:51:46.233763');
INSERT INTO django_admin_log VALUES(36,'3','Outsourcing Commercial Warehouses to a Logistics Partner',2,'[{"changed": {"fields": ["Photo"]}}]',11,1,'2023-12-29 13:57:00.105596');
INSERT INTO django_admin_log VALUES(37,'3','Data-Driven Optimization of Commercial Warehouse Outsourcing',2,'[{"changed": {"fields": ["Title", "Insight", "Body"]}}]',11,1,'2023-12-29 15:08:32.776648');
INSERT INTO django_admin_log VALUES(38,'3','Data-Driven Optimization of Commercial Warehouse Outsourcing',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-29 15:11:12.548419');
INSERT INTO django_admin_log VALUES(39,'3','Data-Driven Optimization of Commercial Warehouse Outsourcing',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-29 15:17:54.938498');
INSERT INTO django_admin_log VALUES(40,'4','Optimizing Shipping Routes to Reduce Lead Time',1,'[{"added": {}}]',11,1,'2023-12-29 16:02:14.222732');
INSERT INTO django_admin_log VALUES(41,'4','Optimizing Shipping Routes to Reduce Lead Time',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-29 16:03:18.356177');
INSERT INTO django_admin_log VALUES(42,'5','Dashboard Development for Business Insights',1,'[{"added": {}}]',11,1,'2023-12-30 05:35:37.459739');
INSERT INTO django_admin_log VALUES(43,'2','Test 2',3,'',11,1,'2023-12-30 05:35:59.411591');
INSERT INTO django_admin_log VALUES(44,'1','TEst 1',3,'',11,1,'2023-12-30 05:35:59.413584');
INSERT INTO django_admin_log VALUES(45,'6','Data Cleaning and Analysis of Movie Database(Sample)',1,'[{"added": {}}]',11,1,'2023-12-30 06:28:16.303121');
INSERT INTO django_admin_log VALUES(46,'6','Data Cleaning and Analysis of Movie Database(Sample)',2,'[{"changed": {"fields": ["Photo"]}}]',11,1,'2023-12-30 06:28:31.489220');
INSERT INTO django_admin_log VALUES(47,'7','A/B Testing Analysis for E-commerce Decision-Making',1,'[{"added": {}}]',11,1,'2023-12-30 07:23:11.945151');
INSERT INTO django_admin_log VALUES(48,'8','Analysis of WeRateDogs Site',1,'[{"added": {}}]',11,1,'2023-12-30 07:42:46.982263');
INSERT INTO django_admin_log VALUES(49,'9','Starbucks capstone project',1,'[{"added": {}}]',11,1,'2023-12-30 08:20:41.421461');
INSERT INTO django_admin_log VALUES(50,'9','Starbucks capstone project',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-30 08:23:35.013605');
INSERT INTO django_admin_log VALUES(51,'9','Starbucks capstone project',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-30 08:32:33.057935');
INSERT INTO django_admin_log VALUES(52,'9','Starbucks capstone project',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-30 08:32:51.846656');
INSERT INTO django_admin_log VALUES(53,'10','Machine Learning Algorithm to Predict Used Cars Prices in Egypt',1,'[{"added": {}}]',11,1,'2023-12-30 09:13:50.305667');
INSERT INTO django_admin_log VALUES(54,'11','Deploying and Monitoring Image Classification Workflow Using Amazon AWS',1,'[{"added": {}}]',11,1,'2023-12-30 09:49:24.233943');
INSERT INTO django_admin_log VALUES(55,'12','Scraping TCG Player Prices for Optimal Deals',1,'[{"added": {}}]',11,1,'2023-12-30 10:06:13.120456');
INSERT INTO django_admin_log VALUES(56,'13','SQL Analysis of online Music store Database',1,'[{"added": {}}]',11,1,'2023-12-30 10:52:18.535769');
INSERT INTO django_admin_log VALUES(57,'14','Exploratory Data Analysis of US Census Demographic Data',1,'[{"added": {}}]',11,1,'2023-12-30 11:07:10.252811');
INSERT INTO django_admin_log VALUES(58,'3','Data-Driven Optimization of Commercial Warehouse Outsourcing',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:36:36.841173');
INSERT INTO django_admin_log VALUES(59,'4','Optimizing Shipping Routes to Reduce Lead Time',2,'[{"changed": {"fields": ["Category", "Body"]}}]',11,1,'2023-12-30 12:37:20.752890');
INSERT INTO django_admin_log VALUES(60,'5','Dashboard Development for Business Insights',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:37:36.878588');
INSERT INTO django_admin_log VALUES(61,'7','A/B Testing Analysis for E-commerce Decision-Making',2,'[]',11,1,'2023-12-30 12:38:14.536378');
INSERT INTO django_admin_log VALUES(62,'8','Analysis of WeRateDogs Site',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:38:57.598609');
INSERT INTO django_admin_log VALUES(63,'9','Starbucks capstone project',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:40:06.852843');
INSERT INTO django_admin_log VALUES(64,'8','Analysis of WeRateDogs Site',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:40:10.599072');
INSERT INTO django_admin_log VALUES(65,'9','Starbucks capstone project',2,'[]',11,1,'2023-12-30 12:40:14.671663');
INSERT INTO django_admin_log VALUES(66,'10','Machine Learning Algorithm to Predict Used Cars Prices in Egypt',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:40:26.921916');
INSERT INTO django_admin_log VALUES(67,'11','Deploying and Monitoring Image Classification Workflow Using Amazon AWS',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:40:44.853993');
INSERT INTO django_admin_log VALUES(68,'12','Scraping TCG Player Prices for Optimal Deals',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:41:02.808089');
INSERT INTO django_admin_log VALUES(69,'13','SQL Analysis of online Music store Database',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:41:15.111204');
INSERT INTO django_admin_log VALUES(70,'14','Exploratory Data Analysis of US Census Demographic Data',2,'[{"changed": {"fields": ["Category"]}}]',11,1,'2023-12-30 12:41:32.705993');
INSERT INTO django_admin_log VALUES(71,'1','Blog Test',1,'[{"added": {}}]',12,1,'2023-12-30 13:59:07.383232');
INSERT INTO django_admin_log VALUES(72,'14','Exploratory Data Analysis of US Census Demographic Data',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-30 16:34:09.552498');
INSERT INTO django_admin_log VALUES(73,'14','Exploratory Data Analysis of US Census Demographic Data',2,'[{"changed": {"fields": ["Body"]}}]',11,1,'2023-12-30 16:34:44.055132');
INSERT INTO django_admin_log VALUES(74,'1','اية الفرق بين ال mean, mode, median',2,'[{"changed": {"fields": ["Title", "Category", "Insight", "Body", "Photo"]}}]',12,1,'2023-12-30 16:42:41.454771');
INSERT INTO django_admin_log VALUES(75,'2','Difference between Mode, Mean, Median',1,'[{"added": {}}]',12,1,'2023-12-30 16:44:24.992774');
INSERT INTO django_admin_log VALUES(76,'1','اية الفرق بين ال mean, mode, median',2,'[{"changed": {"fields": ["Body"]}}]',12,1,'2023-12-30 16:44:32.477500');
INSERT INTO django_admin_log VALUES(77,'1','اية الفرق بين ال المتوسط , الوسط , المنوال',2,'[{"changed": {"fields": ["Title"]}}]',12,1,'2023-12-30 16:44:43.199527');
INSERT INTO django_admin_log VALUES(78,'3','اية هو الانحراف المعياري؟ ليه كنا بنذاكر الحاجات دي؟',1,'[{"added": {}}]',12,1,'2023-12-30 16:49:16.667501');
INSERT INTO django_admin_log VALUES(79,'4','What is the “Standard Deviation?”',1,'[{"added": {}}]',12,1,'2023-12-30 16:50:15.034540');
CREATE TABLE IF NOT EXISTS "django_content_type" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "app_label" varchar(100) NOT NULL, "model" varchar(100) NOT NULL);
INSERT INTO django_content_type VALUES(1,'admin','logentry');
INSERT INTO django_content_type VALUES(2,'auth','permission');
INSERT INTO django_content_type VALUES(3,'auth','group');
INSERT INTO django_content_type VALUES(4,'auth','user');
INSERT INTO django_content_type VALUES(5,'contenttypes','contenttype');
INSERT INTO django_content_type VALUES(6,'sessions','session');
INSERT INTO django_content_type VALUES(7,'homesite','subscriber');
INSERT INTO django_content_type VALUES(8,'homesite','emailtemplate');
INSERT INTO django_content_type VALUES(9,'homesite','subscribetonewsletter');
INSERT INTO django_content_type VALUES(10,'homesite','contact');
INSERT INTO django_content_type VALUES(11,'homesite','portfolio');
INSERT INTO django_content_type VALUES(12,'homesite','blogposts');
CREATE TABLE IF NOT EXISTS "auth_permission" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "content_type_id" integer NOT NULL REFERENCES "django_content_type" ("id") DEFERRABLE INITIALLY DEFERRED, "codename" varchar(100) NOT NULL, "name" varchar(255) NOT NULL);
INSERT INTO auth_permission VALUES(1,1,'add_logentry','Can add log entry');
INSERT INTO auth_permission VALUES(2,1,'change_logentry','Can change log entry');
INSERT INTO auth_permission VALUES(3,1,'delete_logentry','Can delete log entry');
INSERT INTO auth_permission VALUES(4,1,'view_logentry','Can view log entry');
INSERT INTO auth_permission VALUES(5,2,'add_permission','Can add permission');
INSERT INTO auth_permission VALUES(6,2,'change_permission','Can change permission');
INSERT INTO auth_permission VALUES(7,2,'delete_permission','Can delete permission');
INSERT INTO auth_permission VALUES(8,2,'view_permission','Can view permission');
INSERT INTO auth_permission VALUES(9,3,'add_group','Can add group');
INSERT INTO auth_permission VALUES(10,3,'change_group','Can change group');
INSERT INTO auth_permission VALUES(11,3,'delete_group','Can delete group');
INSERT INTO auth_permission VALUES(12,3,'view_group','Can view group');
INSERT INTO auth_permission VALUES(13,4,'add_user','Can add user');
INSERT INTO auth_permission VALUES(14,4,'change_user','Can change user');
INSERT INTO auth_permission VALUES(15,4,'delete_user','Can delete user');
INSERT INTO auth_permission VALUES(16,4,'view_user','Can view user');
INSERT INTO auth_permission VALUES(17,5,'add_contenttype','Can add content type');
INSERT INTO auth_permission VALUES(18,5,'change_contenttype','Can change content type');
INSERT INTO auth_permission VALUES(19,5,'delete_contenttype','Can delete content type');
INSERT INTO auth_permission VALUES(20,5,'view_contenttype','Can view content type');
INSERT INTO auth_permission VALUES(21,6,'add_session','Can add session');
INSERT INTO auth_permission VALUES(22,6,'change_session','Can change session');
INSERT INTO auth_permission VALUES(23,6,'delete_session','Can delete session');
INSERT INTO auth_permission VALUES(24,6,'view_session','Can view session');
INSERT INTO auth_permission VALUES(25,7,'add_subscriber','Can add subscriber');
INSERT INTO auth_permission VALUES(26,7,'change_subscriber','Can change subscriber');
INSERT INTO auth_permission VALUES(27,7,'delete_subscriber','Can delete subscriber');
INSERT INTO auth_permission VALUES(28,7,'view_subscriber','Can view subscriber');
INSERT INTO auth_permission VALUES(29,8,'add_emailtemplate','Can add email template');
INSERT INTO auth_permission VALUES(30,8,'change_emailtemplate','Can change email template');
INSERT INTO auth_permission VALUES(31,8,'delete_emailtemplate','Can delete email template');
INSERT INTO auth_permission VALUES(32,8,'view_emailtemplate','Can view email template');
INSERT INTO auth_permission VALUES(33,9,'add_subscribetonewsletter','Can add subscribe to newsletter');
INSERT INTO auth_permission VALUES(34,9,'change_subscribetonewsletter','Can change subscribe to newsletter');
INSERT INTO auth_permission VALUES(35,9,'delete_subscribetonewsletter','Can delete subscribe to newsletter');
INSERT INTO auth_permission VALUES(36,9,'view_subscribetonewsletter','Can view subscribe to newsletter');
INSERT INTO auth_permission VALUES(37,10,'add_contact','Can add contact');
INSERT INTO auth_permission VALUES(38,10,'change_contact','Can change contact');
INSERT INTO auth_permission VALUES(39,10,'delete_contact','Can delete contact');
INSERT INTO auth_permission VALUES(40,10,'view_contact','Can view contact');
INSERT INTO auth_permission VALUES(41,11,'add_portfolio','Can add portfolio');
INSERT INTO auth_permission VALUES(42,11,'change_portfolio','Can change portfolio');
INSERT INTO auth_permission VALUES(43,11,'delete_portfolio','Can delete portfolio');
INSERT INTO auth_permission VALUES(44,11,'view_portfolio','Can view portfolio');
INSERT INTO auth_permission VALUES(45,12,'add_blogposts','Can add blog posts');
INSERT INTO auth_permission VALUES(46,12,'change_blogposts','Can change blog posts');
INSERT INTO auth_permission VALUES(47,12,'delete_blogposts','Can delete blog posts');
INSERT INTO auth_permission VALUES(48,12,'view_blogposts','Can view blog posts');
CREATE TABLE IF NOT EXISTS "auth_group" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "name" varchar(150) NOT NULL UNIQUE);
CREATE TABLE IF NOT EXISTS "auth_user" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "password" varchar(128) NOT NULL, "last_login" datetime NULL, "is_superuser" bool NOT NULL, "username" varchar(150) NOT NULL UNIQUE, "last_name" varchar(150) NOT NULL, "email" varchar(254) NOT NULL, "is_staff" bool NOT NULL, "is_active" bool NOT NULL, "date_joined" datetime NOT NULL, "first_name" varchar(150) NOT NULL);
INSERT INTO auth_user VALUES(1,'pbkdf2_sha256$600000$qXXeTxcr4NpJujYHC0czpl$wmkmbrXMxmpYGhHWXm+9DZKwhbrWH1wejt89zQHM/G8=','2023-12-22 13:56:00.406818',1,'mina','','mina@gmail.com',1,1,'2023-12-06 08:43:05.749461','');
CREATE TABLE IF NOT EXISTS "django_session" ("session_key" varchar(40) NOT NULL PRIMARY KEY, "session_data" text NOT NULL, "expire_date" datetime NOT NULL);
INSERT INTO django_session VALUES('jvmtij0jpg4h2d0igj1rxmbr2sjf5ox7','.eJxVjEEOwiAQRe_C2hAUZgCX7nsGMjBUqgaS0q6Md7dNutDtf-_9twi0LiWsPc9hYnEVZ3H63SKlZ6474AfVe5Op1WWeotwVedAuh8b5dTvcv4NCvWy1t6BGBzwiO81oGYgRIUfKDjxYMJijRnJaRX1Jhq3fbGOUNwkVG_H5At4ON2c:1rAnUz:b16fiX-c8ie2n71cAF6rmKRrfyhCAOZCptgGoA_W214','2023-12-20 08:43:17.664762');
INSERT INTO django_session VALUES('a3cax1umil59nnidkzwg7ioqien6bkvn','.eJxVjEEOwiAQRe_C2hAUZgCX7nsGMjBUqgaS0q6Md7dNutDtf-_9twi0LiWsPc9hYnEVZ3H63SKlZ6474AfVe5Op1WWeotwVedAuh8b5dTvcv4NCvWy1t6BGBzwiO81oGYgRIUfKDjxYMJijRnJaRX1Jhq3fbGOUNwkVG_H5At4ON2c:1rGg0O:R4Ab3aoldMVgtvfbOo_1hnKbmqWHQ8yUV6ANGXwzdJQ','2024-01-05 13:56:00.408812');
CREATE TABLE IF NOT EXISTS "homesite_subscribetonewsletter" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "email" varchar(254) NOT NULL UNIQUE, "subscribed_at" datetime NOT NULL);
INSERT INTO homesite_subscribetonewsletter VALUES(1,'mina@gmail.com','2023-12-06 11:17:28.252452');
INSERT INTO homesite_subscribetonewsletter VALUES(2,'mina2@gmail.com','2023-12-06 12:01:20.031835');
INSERT INTO homesite_subscribetonewsletter VALUES(3,'mina3@gmail.com','2023-12-06 12:01:32.156597');
INSERT INTO homesite_subscribetonewsletter VALUES(4,'ff@fd.com','2023-12-07 09:22:47.448681');
INSERT INTO homesite_subscribetonewsletter VALUES(5,'msnessim@gmail.com','2023-12-07 10:35:45.688024');
INSERT INTO homesite_subscribetonewsletter VALUES(6,'fffff@ff.ff','2023-12-22 14:00:54.940075');
INSERT INTO homesite_subscribetonewsletter VALUES(7,'mm@g.com','2023-12-25 05:19:33.723561');
CREATE TABLE IF NOT EXISTS "homesite_contact" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "name" varchar(100) NOT NULL, "email" varchar(254) NOT NULL, "subject" varchar(100) NOT NULL, "message" text NOT NULL, "sent_at" datetime NOT NULL);
INSERT INTO homesite_contact VALUES(1,'mina','nessim@m.cm','hello world!','his this is the message','2023-12-07 13:29:59.994004');
INSERT INTO homesite_contact VALUES(2,'mia','MIN@EF.ovm','helo tany','fdsfdsfdsf','2023-12-07 14:48:45.958066');
INSERT INTO homesite_contact VALUES(3,'Mina Nessim','mina.s.nessim@gmail.com','dfdf','sdfsd','2023-12-07 14:58:32.792238');
INSERT INTO homesite_contact VALUES(4,'Mina Nessim','mina.s.nessim@gmail.com','fffffffff','dddddd','2023-12-07 15:02:03.331082');
INSERT INTO homesite_contact VALUES(5,'Mina Nessim','mina.s.nessim@gmail.com','ffds','sd','2023-12-07 15:25:19.848060');
INSERT INTO homesite_contact VALUES(6,'Mina Soliman Nessim','mina.s.nessim@gmail.com','aaa','aa','2023-12-07 15:25:40.703152');
INSERT INTO homesite_contact VALUES(7,'Mina Soliman Nessim','mina.s.nessim@gmail.com','aa','a','2023-12-07 15:28:35.190549');
INSERT INTO homesite_contact VALUES(8,'Mina Nessim','mina.s.nessim@gmail.com','dsd','ds','2023-12-07 15:28:48.000834');
INSERT INTO homesite_contact VALUES(9,'Mina Soliman Nessim','mina.s.nessim@gmail.com','sssss','ssssss','2023-12-07 15:30:23.528846');
INSERT INTO homesite_contact VALUES(10,'Mina Soliman Nessim','mina.s.nessim@gmail.com','1','df','2023-12-07 15:33:49.248927');
INSERT INTO homesite_contact VALUES(11,'Mina Soliman Nessim','mina.s.nessim@gmail.com','2','dsa','2023-12-07 15:34:06.345052');
INSERT INTO homesite_contact VALUES(12,'Mina Soliman Nessim','mina.s.nessim@gmail.com','3','sd','2023-12-07 15:34:42.817192');
INSERT INTO homesite_contact VALUES(13,'Mina Soliman Nessim','ff@fd.com','5','f','2023-12-07 15:37:53.999122');
INSERT INTO homesite_contact VALUES(14,'Mina Soliman Nessim','mina.s.nessim@gmail.com','f','f','2023-12-07 15:41:09.082499');
INSERT INTO homesite_contact VALUES(15,'Mina Soliman Nessim','mina.s.nessim@gmail.com','hello world!','f','2023-12-07 15:42:01.384072');
INSERT INTO homesite_contact VALUES(16,'kmif','mid@df.kdf','mimdf',replace(replace('m;klfds\r\ncxxz','\r',char(13)),'\n',char(10)),'2023-12-22 14:12:42.361310');
INSERT INTO homesite_contact VALUES(17,'mmm','nnn@nnn.nnn','hello again 25 dec','decdecdec','2023-12-25 05:19:14.150258');
CREATE TABLE IF NOT EXISTS "homesite_portfolio" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "title" varchar(100) NOT NULL, "category" varchar(100) NOT NULL, "photo" varchar(100) NULL, "insight" varchar(259) NOT NULL, "body" text NOT NULL);
INSERT INTO homesite_portfolio VALUES(3,'Data-Driven Optimization of Commercial Warehouse Outsourcing','filter-dashboard,filter-excel,filter-supply_chain','portfolio_photos/Warehouse.jpg','Strategically leveraging data analytics for streamlined warehouse outsourcing, achieving cost reduction and operational enhancements.',replace(replace('<div>\r\n<p>&nbsp;</p>\r\n\r\n<p><span style="color:#374151"><strong>&quot;Please note that specific numbers and data have been intentionally omitted from this portfolio for the company&#39;s confidentiality.&quot;</strong></span></p>\r\n\r\n<div>\r\n<p><strong><span style="color:black">Project Overview:</span></strong> <span style="color:black">The objective centered on outsourcing commercial warehouses to heighten efficiency, productivity, and cost-effectiveness in tandem with meeting business demands. This initiative aimed to optimize warehouse operations through data-driven strategies.</span></p>\r\n\r\n<p><strong><span style="color:black">Role as a Data Analyst:</span></strong></p>\r\n</div>\r\n\r\n<div style="margin-left:24px">\r\n<ul>\r\n	<li><span style="color:black"><strong>Data Management:</strong> Ensured accuracy and consistency between warehouse software systems.</span></li>\r\n	<li><span style="color:black"><strong>Performance Analysis:</strong> Conducted data-driven analyses to identify efficiencies and cost-saving opportunities.</span></li>\r\n	<li><span style="color:black"><strong>Reporting:</strong> Generated comprehensive reports, graphs, and visualizations for stakeholder communication.</span></li>\r\n	<li><span style="color:black"><strong>Interface Development:</strong> Collaborated on interface development to facilitate seamless data exchange.</span></li>\r\n	<li><span style="color:black"><strong>Detailed Warehouse Process and Control Establishment:</strong> Implemented structured processes for efficient warehouse operations.</span></li>\r\n</ul>\r\n</div>\r\n\r\n<div>\r\n<p><strong><span style="color:black">Data-Driven Outcomes:</span></strong></p>\r\n</div>\r\n\r\n<div style="margin-left:24px">\r\n<ul>\r\n	<li><span style="color:black"><strong>Cost Optimization:</strong> Utilized data insights to achieve a 50%+ cost reduction, aligning with forecasts.</span></li>\r\n	<li><span style="color:black"><strong>Invoice Processing:</strong> Marked a substantial reduction post-migration.</span></li>\r\n	<li><span style="color:black"><strong>Operational Enhancements:</strong> Addressed bottlenecks, resulting in reduced headcount, improved security, and operational loss prevention.</span></li>\r\n</ul>\r\n</div>\r\n\r\n<div>\r\n<p><strong><span style="color:black">Project Duration:</span></strong> <span style="color:black">Spanned approximately 1.5 years, involving stocktaking, shipping, and reconciliations.</span></p>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/29/outsourcing-wh-1.PNG" style="height:323px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/29/outsourcing-wh-2.PNG" style="height:331px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/29/outsourcing-wh-3.png" style="height:345px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/29/outsourcing-wh-4.PNG" style="height:380px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/29/outsourcing-wh-5.PNG" style="height:328px; width:600px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n</div>\r\n</div>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(4,'Optimizing Shipping Routes to Reduce Lead Time','filter-dashboard,filter-excel,filter-supply_chain','portfolio_photos/WH4.jpg','Strategic implementation of CPFR aimed at shifting network equipment replenishment from China to Dubai. The primary goal was to heighten operational efficiency and reduce costs significantly by streamlining lead times.',replace(replace('<div>\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>&quot;Please note that specific numbers and data have been intentionally omitted from this portfolio for the the company&#39;s confidentiality.&quot;</strong></p>\r\n\r\n<p><strong><span style="color:black">Project Objective:</span></strong></p>\r\n</div>\r\n\r\n<div style="margin-left:24px">\r\n<ul>\r\n	<li><span style="color:black">Implemented CPFR (Collaborative Planning Forecasting and Replenishment) to transition network equipment replenishment from China to Dubai.</span></li>\r\n	<li><span style="color:black">Objective: Enhance operational efficiency and decrease costs by reducing lead time.</span></li>\r\n</ul>\r\n</div>\r\n\r\n<div>\r\n<p><strong><span style="color:black">Role as a Data Analyst:</span></strong></p>\r\n</div>\r\n\r\n<div style="margin-left:24px">\r\n<ul>\r\n	<li><span style="color:black"><strong>Data-Driven Decision Making:</strong> Employed data analysis to forecast demand, improving accuracy in equipment requisitions and rollout plans.</span></li>\r\n	<li><span style="color:black"><strong>Performance Monitoring:</strong> Generated reports and visualizations to track shipment timelines and inventory movement.</span></li>\r\n	<li><span style="color:black"><strong>Process Enhancement:</strong> Collaborated on developing tools for managing stock levels, ensuring optimal inventory and timely shipments.</span></li>\r\n	<li><span style="color:black"><strong>Interface Development:</strong> Facilitated seamless data exchange between teams to align shipments with updated plans.</span></li>\r\n</ul>\r\n</div>\r\n\r\n<div>\r\n<p><strong><span style="color:black">Data-Backed Achievements:</span></strong></p>\r\n</div>\r\n\r\n<div style="margin-left:24px">\r\n<ul>\r\n	<li><span style="color:black"><strong>Lead Time Reduction:</strong> Implemented strategies resulting in a notable 4&ndash;5-week reduction in lead time via improved forecasting and agile response to rollout plans.</span></li>\r\n	<li><span style="color:black"><strong>Inventory &amp; Cash Flow Optimization:</strong> Used data insights to synchronize equipment shipments with consumption, improving stock quality and optimizing cash flow management.</span></li>\r\n	<li><span style="color:black"><strong>Customization &amp; Flexibility:</strong> Employed data forecasts to customize packaging and adjust plans, enhancing flexibility and responsiveness to demand changes.</span></li>\r\n	<li><span style="color:black"><strong>Process Improvement:</strong> Established a data-driven planning tool and streamlined tracking mechanisms for timely and efficient supply.</span></li>\r\n</ul>\r\n</div>\r\n\r\n<div>\r\n<p><strong><span style="color:black">Benefits Realized:</span></strong></p>\r\n</div>\r\n\r\n<div style="margin-left:24px">\r\n<ul>\r\n	<li><span style="color:black"><strong>Operational Efficiency:</strong> Improved lead times, granting OEG technical teams up to 40 days flexibility in configuration adjustments before delivery.</span></li>\r\n	<li><span style="color:black"><strong>Work-in-Process Reduction:</strong> Eliminated excess stock, optimizing WIP and reducing operational expenditures in warehouses.</span></li>\r\n	<li><span style="color:black"><strong>Financial Impact:</strong> Reduced the cash-to-cash cycle, enhancing return on assets and alleviating pressure on foreign currency payments to suppliers.</span></li>\r\n	<li><span style="color:black"><strong>Operational Flexibility:</strong> Enabled customization based on OEG requirements in Dubai, ensuring a faster spare parts replenishment cycle.</span></li>\r\n</ul>\r\n\r\n<p>Change in Route:</p>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/29/wh1.png" style="height:306px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/29/wh3.PNG" style="height:262px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/29/wh2.PNG" style="height:305px; width:600px" /></p>\r\n\r\n<p><span style="color:#000000">Example of tools &amp; reports </span></p>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/29/wh-5.png" style="height:315px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/29/wh7.PNG" style="height:199px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/29/wh-6.PNG" style="height:246px; width:600px" /></p>\r\n</div>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(5,'Dashboard Development for Business Insights','filter-dashboard,filter-excel,filter-supply_chain','portfolio_photos/DB_3.png','Creation of comprehensive dashboards providing vital business insights through graphical representations and data visualization.',replace(replace('<p>&nbsp;</p>\r\n\r\n<p><strong>&quot;Please note that specific numbers and data have been intentionally omitted from this portfolio for the the company&#39;s confidentiality.&quot;</strong></p>\r\n\r\n<p><strong>Project Objective:</strong></p>\r\n\r\n<ul>\r\n	<li>Develop interactive dashboards to offer clear visualizations aiding in decision-making processes.</li>\r\n	<li>Provide stakeholders with a user-friendly interface for accessing and interpreting key business metrics.</li>\r\n</ul>\r\n\r\n<p><strong>Role in Dashboard Development:</strong></p>\r\n\r\n<ul>\r\n	<li><strong>Data Integration:</strong> Collated and integrated diverse data sources for unified insights.</li>\r\n	<li><strong>Dashboard Design:</strong> Structured visually appealing and intuitive dashboards catering to specific stakeholder needs.</li>\r\n	<li><strong>Visualization Optimization:</strong> Ensured optimal data representation using graphs, charts, and tables.</li>\r\n	<li><strong>User Experience Enhancement:</strong> Streamlined user interface and functionality for easy navigation and understanding.</li>\r\n</ul>\r\n\r\n<p><strong>Dashboard Impact:</strong></p>\r\n\r\n<ul>\r\n	<li><strong>Improved Decision Making:</strong> Facilitated informed decisions through easily accessible and interpretable data representations.</li>\r\n	<li><strong>Enhanced Data Accessibility:</strong> Enabled stakeholders to access critical information swiftly and efficiently.</li>\r\n	<li><strong>Data-Backed Insights:</strong> Empowered teams with actionable insights, optimizing processes and strategies.</li>\r\n</ul>\r\n\r\n<p>Below as some examples, i ommited some numbers and labels for privacy</p>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/30/db-1.PNG" style="height:372px; width:599px" /><img alt="" src="/media/content/ckeditor/2023/12/30/db-2.PNG" style="height:395px; width:570px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/30/db-3.png" style="height:351px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/db-4.PNG" style="height:369px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/db-5.PNG" style="height:384px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/db-6.PNG" style="height:354px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/db-7.PNG" style="height:437px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/db-8.PNG" style="height:336px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/db-9.PNG" style="height:330px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/db-10.PNG" style="height:320px; width:596px" /></p>\r\n\r\n<p>&nbsp;</p>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(6,'Data Cleaning and Analysis of Movie Database(Sample)','filter-python','portfolio_photos/CD_5.png','Exploring a comprehensive dataset from TMDb with 10.8K movies. This project aims to clean, analyze movie attributes like ratings, revenue, and genres to extract industry-related insights.',replace(replace('<p>&nbsp;</p>\r\n\r\n<p><strong>Introduction:</strong></p>\r\n\r\n<ul>\r\n	<li>Overview of the dataset collected from The Movie Database (TMDb), encompassing information on 10.8K movies across 21 columns.</li>\r\n	<li>Description of the dataset context and variables included.</li>\r\n</ul>\r\n\r\n<p><strong>Key Steps in Data Cleaning:</strong></p>\r\n\r\n<ol>\r\n	<li><strong>Data Import:</strong> Load the dataset into Python environment.</li>\r\n	<li><strong>Preliminary Exploration:</strong> Understand the structure, datatypes, and summary statistics of the dataset.</li>\r\n	<li><strong>Handling Missing Values:</strong> Identify and handle missing or null values in the dataset.</li>\r\n	<li><strong>Data Consistency:</strong> Check for inconsistencies, errors, or duplicates in the data.</li>\r\n	<li><strong>Standardization and Formatting:</strong> Standardize column names, format data types, and address any outliers.</li>\r\n	<li><strong>Data Transformation:</strong> Perform necessary transformations (if any) for better analysis.</li>\r\n	<li><strong>Final Dataset:</strong> Present the cleaned dataset for further analysis.</li>\r\n</ol>\r\n\r\n<p><strong>Exploratory Data Analysis (EDA) Questions:</strong></p>\r\n\r\n<ol>\r\n	<li><strong>Dependent Variable:</strong>\r\n\r\n	<ul>\r\n		<li>Identify a dependent variable for analysis.</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Independent Variables (At least three to explore):</strong>\r\n	<ul>\r\n		<li>Consider exploring variables.</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Extracting insights from the Data</strong></li>\r\n</ol>\r\n\r\n<p><strong>Analysis and Visualization:</strong></p>\r\n\r\n<ul>\r\n	<li>Utilize Python libraries like Pandas, Matplotlib, or Seaborn for analysis and visualization.</li>\r\n	<li>Generate visual representations (e.g., graphs, charts) to illustrate findings.</li>\r\n</ul>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/30/cd-1.png" style="height:189px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/cd-2.png" style="height:222px; width:600px" /></p>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/30/cd-3.png" style="height:517px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/cd-4.png" style="height:519px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/cd-5.png" style="height:657px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/cd-6.png" style="height:592px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/cd-7.PNG" style="height:292px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/cd-8.PNG" style="height:583px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/cd-9.png" style="height:226px; width:600px" /><img alt="" src="/media/content/ckeditor/2023/12/30/cd-10.png" style="height:238px; width:600px" /></p>\r\n\r\n<p>&nbsp;</p>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(7,'A/B Testing Analysis for E-commerce Decision-Making','filter-python,filter-statistic','portfolio_photos/AB_1.png','Engaging in an A/B test analysis for an e-commerce platform, aimed at deciphering outcomes between implementing a new/old  page, or extending the experiment. Addressing complexities in A/B tests, the project seeks clarity for informed decision-making.',replace(replace('<p>&nbsp;</p>\r\n\r\n<p><strong>Objective:</strong></p>\r\n\r\n<ul>\r\n	<li>Analyze results from an A/B test conducted by an e-commerce site.</li>\r\n	<li>Aid the company in deciding between new and old page implementation based on conclusive insights derived from the analysis.</li>\r\n	<li>Follow along with the corresponding quiz questions, ensuring alignment with classroom concepts for meeting project criteria.</li>\r\n</ul>\r\n\r\n<p><strong>Key Sections:</strong></p>\r\n\r\n<ol>\r\n	<li><strong>Part I - Probability:</strong>\r\n\r\n	<ul>\r\n		<li>Exploring probability concepts and formulating null and alternative hypotheses.</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Part II - A/B Test:</strong>\r\n	<ul>\r\n		<li>Considering duration and consistency in A/B test significance.</li>\r\n		<li>Conducting simulations and sampling distributions for decision-making.</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Part III - Regression Approach:</strong>\r\n	<ul>\r\n		<li>Utilizing logistic regression for analyzing conversion rates.</li>\r\n		<li>Creating dummy variables and fitting regression models to ascertain page impact on conversions.</li>\r\n	</ul>\r\n	</li>\r\n</ol>\r\n\r\n<p><strong>Conclusion:</strong></p>\r\n\r\n<ul>\r\n	<li>Inconclusive significance suggests retention of the old page.</li>\r\n</ul>\r\n\r\n<p><strong>Sample from the analysis:</strong></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112209-1.png" style="height:121px; width:294px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Cleaning data</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112209-2.png" style="height:124px; width:720px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Extracting statistics</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-3.png" style="height:100px; width:515px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-4.png" style="height:106px; width:531px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-5.png" style="height:110px; width:566px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-6.png" style="height:97px; width:511px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-7.png" style="height:113px; width:283px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-8.png" style="height:112px; width:303px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-9.png" style="height:115px; width:482px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-10.png" style="height:115px; width:486px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-11.png" style="height:109px; width:493px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-12.png" style="height:110px; width:493px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-13.png" style="height:109px; width:541px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-14.png" style="height:157px; width:622px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-15.png" style="height:429px; width:481px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-16.png" style="height:137px; width:583px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-17.png" style="height:74px; width:305px" /></p>\r\n\r\n<p><strong><span style="background-color:white"><span style="color:black">I calculated the P-value, this huge value indicates that old page is better than the new page (alpha is .05) so we fail to reject the null hypothesis</span></span></strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-18.png" style="height:130px; width:541px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-19.png" style="height:91px; width:820px" /></p>\r\n\r\n<h3><span style="background-color:white"><span style="color:#1f3763">Part III - A regression approach</span></span></h3>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-20.png" style="height:300px; width:662px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-21.png" style="height:134px; width:504px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-22.png" style="height:450px; width:635px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-23.png" style="height:700px; width:610px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-24.png" style="height:83px; width:329px" /><img src="/media/content/ckeditor/2023/12/30/image-20231230112238-25.png" style="height:698px; width:824px" /></p>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(8,'Analysis of WeRateDogs Site','filter-python,filter-statistic,filter-machine_learning,filter-scrapping','portfolio_photos/1_KinIUBsckpMYvuBganlgsw.png','Quick analysis for different data sets related to WeRateDogs page on twitter, including Data Scrapping, cleaning, visualization and insights.',replace(replace('<p>&nbsp;</p>\r\n\r\n<h2><strong>Introduction:</strong></h2>\r\n\r\n<p>As of today, WeRateDogs has 8.8M follower on twitter, it gained this publicity by rating peoples dogs with a good sense of humor, with the rating process they sometimes give more than 10/10, so it&rsquo;s normal to see a dog with 14/10 rating, So some cleaning for the data will be required.</p>\r\n\r\n<h2><strong>Distribution Rating:</strong></h2>\r\n\r\n<h2><img src="/media/content/ckeditor/2023/12/30/image.png" style="height:378px; width:563px" /></h2>\r\n\r\n<p>As we can see in the distribution rating histogram, data is left skewed, as 75% of rates are above 10/10, which gives the idea that rating is used in funny way more than actual rating from 10</p>\r\n\r\n<h2><strong>Dogs Stages:</strong></h2>\r\n\r\n<h2><img src="/media/content/ckeditor/2023/12/30/image_wEWZfps.png" style="height:384px; width:554px" /></h2>\r\n\r\n<h2>&nbsp;</h2>\r\n\r\n<p>It shows that pupper is most common dog stage within the shared dogs, excluding dogs with no stage mentioned of course, and then doggo comes in the second place.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><strong>Dog Breeds:</strong></h2>\r\n\r\n<p>Then moving from dog stages to dog breeds, we will find below top 10 dog breeds mentioned in the tweets, worth noticing that golden retriever is much popular than others dog breeds, then comes after it, Labrador retriever, Pembroke, Chihuahua</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_FBqAGOQ.png" style="height:340px; width:616px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><strong>Favorites, Retweets relationship:</strong></h2>\r\n\r\n<p>The graph below presents very interesting points, which is the relation between favorites and retweets, we can notice that there is a strong positive relationship between both retweets and favorites, the more tweet has favorite the more it&rsquo;s retweeted, and this is logical, because it means the tweet is loved by many people, and this is presented in the way of reaction to the post.</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_h8wT09x.png" style="height:360px; width:523px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><strong>Favorites, rating relationship:</strong></h2>\r\n\r\n<p>Same concept as before, the positive relationship between rating and favorite, but this time it&rsquo;s a little bit different, because in the previous graph, both actions were done by the viewer, this time one action is from the page side, which indicates, that the rating given by the page, leads the favorite count in a positive way.</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_vQPQbXs.png" style="height:572px; width:604px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><strong>Dog Image Prediction:</strong></h2>\r\n\r\n<p>This graph represents the percentage of dogs which were identified by machine learning software, to determine the type of the dog. Around 60% of the dogs were identified as dogs by the software, while another 23% might be dogs, the rest could not be identified as dogs at all, which gives us insight about the development of machine learning in this field, 60% is a good percentage, but still not good enough to be afraid from machine revolution against humanity.</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_HR0vsXU.png" style="height:419px; width:467px" /></p>\r\n\r\n<p><strong>Finally:</strong></p>\r\n\r\n<p>Difference between data accuracy before and after data wrangling is really amazing, which explain the idea of the importance of data wrangling</p>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(9,'Starbucks capstone project','filter-python,filter-statistic,filter-machine_learning','portfolio_photos/Starbucks.png','The project analyzes data of 30 days related to 17K customers and 10 offers provided by Starbucks, the  main goal is to find a good machine learning model to predict either the customer will buy a product to  use the offer or not.',replace(replace('<h2>&nbsp;</h2>\r\n\r\n<h2><strong>Problem Statement:</strong></h2>\r\n\r\n<p>While navigating this project, the pivotal hurdle involved transforming diverse and varied data into a format conducive to machine learning analysis. Challenges encompassed merging disparate data frames, handling categorical variables effectively, and determining the optimal approach for dealing with offer-related information. Balancing the use of offer IDs for directness and offer details for predictive potential posed a significant dilemma. Resolving the issue of separate transactional and offer-related data involved merging and categorizing information for streamlined analysis. The ultimate objective was to create a predictive model leveraging customer demographics to forecast offer completion, facilitating informed decision-making in targeted offer distributions.</p>\r\n\r\n<h2><strong>Metrics:</strong></h2>\r\n\r\n<p>We will use accuracy for this problem to define if the machine learning model is effective or not. After defining the best model, we will test accuracy, fbeta, recall ,precision.</p>\r\n\r\n<h2><strong>Data Exploration:</strong></h2>\r\n\r\n<p>After importing the data, I started with basic exploration for the data, and find the simple relation in the row data, there are 17K customers and 10 offers, each customer has his age, gender, income mentioned, with some customers with no data, Also for the offers there are the offer type and broadcasting method Transaction file is the most complicated, as it has 4 types of events (transaction, offer received, offer viewed, and offer completed), offers and transactions are not connected with offer ID, that makes it hard to connect together.</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_pc8iaDo.png" style="height:536px; width:700px" /></p>\r\n\r\n<h2><strong>Exploratory Visualization:</strong></h2>\r\n\r\n<h2><img src="/media/content/ckeditor/2023/12/30/image_haf3RGR.png" style="height:517px; width:800px" /></h2>\r\n\r\n<h2><img src="/media/content/ckeditor/2023/12/30/image_X6hWUb6.png" style="height:632px; width:800px" /></h2>\r\n\r\n<h2><strong>Data Processing:</strong></h2>\r\n\r\n<p>After data exploration, I started to work on merging the 3 data frames to get one data frame containing all important information Then I started to merged transactions with offer completed events, as they should share same customer and same time and be on ordered, Then creating a for loop to check per customer, and mark the received transaction either it&rsquo;s received only, or received, viewed but not done, received viewed and done, received and done but not viewed, or received, done but viewed after the transaction was done This was the longest step in the processing After that I dropped unneeded rows. So the machine learning model would work only on related events Then I created dummy variables for all categorial variables, standardized the continues variables , then dropped all unneeded columns to avoid redundant information, After that I started to apply and try the machine leaning techniques to get the best one.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><strong>Algorithms and Techniques:</strong></h2>\r\n\r\n<p>This problem is a classification supervised problem, for that I needed to find relative models that can help in this problem, so I choose the below 4 models.</p>\r\n\r\n<p>AdaBoostClassifier, GaussianNB, DecisionTreeClassifier, RandomForestClassifier</p>\r\n\r\n<p>After trying each of them with default parameters the results for training and testing were as below</p>\r\n\r\n<p><strong>Training with AdaBoostClassifier</strong></p>\r\n\r\n<p>Training Accuracy: 0.8778</p>\r\n\r\n<p>Testing Accuracy : 0.8753</p>\r\n\r\n<p><strong>Training with GaussianNB</strong></p>\r\n\r\n<p>Training Accuracy: 0.72981</p>\r\n\r\n<p>Testing Accuracy : 0.7286</p>\r\n\r\n<p><strong>Training with DecisionTreeClassifier</strong></p>\r\n\r\n<p>Training Accuracy: 1.0</p>\r\n\r\n<p>Testing Accuracy : 0.8266</p>\r\n\r\n<p><strong>Training with RandomForestClassifier</strong></p>\r\n\r\n<p>Training Accuracy: 0.9999</p>\r\n\r\n<p>Testing Accuracy : 0.8677</p>\r\n\r\n<p><strong>Random forest classifier </strong>was chosen as the one to continue with, another good option would be the Ada Boost Classifier, I choose random forest as the testing accuracy is too close and the training accuracy is much better.</p>\r\n\r\n<p><strong>For hyperparameter tuning</strong>, using GridSearchCV would take hours and too much computational power, so instead I I ran RandomizedSearchCV for the hyper parameter tuning.</p>\r\n\r\n<p>These are the hyperparameters</p>\r\n\r\n<p><em>params = { &#39;n_estimators&#39;: [75, 100, 125, 150], &#39;max_features&#39;: [&#39;sqrt&#39;, &#39;log2&#39;, None], &#39;max_depth&#39; : [10,20,30,40,50], &#39;min_samples_split&#39; : [2,10,15,20], &#39;min_samples_leaf&#39; :[1,2,5,10]}</em></p>\r\n\r\n<p>Then I chose number of iterations to be 200</p>\r\n\r\n<p>After running the randomized search, it gave the best parameters</p>\r\n\r\n<p><em>{&#39;n_estimators&#39;: 75, &#39;min_samples_split&#39;: 15, &#39;min_samples_leaf&#39;: 10, &#39;max_features&#39;: None, &#39;max_depth&#39;: 10}</em></p>\r\n\r\n<p>So after running the model again the new accuracy for the model is</p>\r\n\r\n<p>Training Accuracy: 0.8854</p>\r\n\r\n<p>Testing Accuracy : 0.8791</p>\r\n\r\n<h2><strong>Benchmark:</strong></h2>\r\n\r\n<p>To be able to compare the result, I ran hyperparameter tuning to Ada boost classifier to get another result to be able to benchmark with it</p>\r\n\r\n<p>After running hyperparameter tuning</p>\r\n\r\n<p><em>params = { &#39;n_estimators&#39;: [40, 45, 50, 55, 60], &#39;learning_rate&#39; : [0.8,0.9,1,1.1,1.2], &#39;algorithm&#39;:[&#39;SAMME&#39;, &#39;SAMME.R&#39;] }</em></p>\r\n\r\n<p>The best parameters are<em> {&#39;n_estimators&#39;: 60, &#39;learning_rate&#39;: 1.1, &#39;algorithm&#39;: &#39;SAMME.R&#39;}</em></p>\r\n\r\n<p>And the results are</p>\r\n\r\n<p>Training Accuracy: 0.8772</p>\r\n\r\n<p>Testing Accuracy : 0.8745</p>\r\n\r\n<p>The random forest classifier is slightly better than Ada boost classifier with about .5%</p>\r\n\r\n<h2><strong>Implementing process:</strong></h2>\r\n\r\n<p>To be able to implement the model, data needed to be precise for the machine learning model, having separate lines with different events like offer received and offer viewed doesn&rsquo;t provide a clear out put f or the model to work on, there should be a clear features to test and clear target to predict,</p>\r\n\r\n<p>That&rsquo;s why merging the data into on line per offer, dropping non related transactions was a good option for the last form of the data.</p>\r\n\r\n<p>After that I needed to find a good model for supervised classification, as mentioned in the previous step I tried some of them, but random forest provided a good result.</p>\r\n\r\n<h2><strong>Improving process:</strong></h2>\r\n\r\n<p>Improving the results has two parts Finding the best model and find the best hyperparameters through hyperparameter tuning Fist I tried 4 different models, and got the best 2 out of them To determine which of them is better, I tried hyperparameters tuning for both models With random grid CV, I got the best hyperparameters for both models, as mentioned before And then I created a model for both and got the accuracy scores</p>\r\n\r\n<h2><strong>Results:</strong></h2>\r\n\r\n<p>After I got the two models, I created another test to get the accuracy score, fbeta, recall and precision scores and compare both models to each other</p>\r\n\r\n<p>Results were</p>\r\n\r\n<p><strong>For random forest</strong></p>\r\n\r\n<p>Testing Accuracy : 0.8789</p>\r\n\r\n<p>Testing Fbeta : 0.8749</p>\r\n\r\n<p>Testing Recall : 0.9263</p>\r\n\r\n<p>Testing Precision : 0.8064</p>\r\n\r\n<p><strong>For ada boost</strong></p>\r\n\r\n<p>Testing Accuracy : 0.8745</p>\r\n\r\n<p>Testing Fbeta : 0.8698</p>\r\n\r\n<p>Testing Recall : 0.8922</p>\r\n\r\n<p>Testing Precision : 0.8176</p>\r\n\r\n<p><strong>Random forest</strong> is better for overall scores, for 3 out of 4 scores, yet the only one is for precision, yet the difference is only 1%</p>\r\n\r\n<p>To make sure of the results, tried same model with different seeds and got the below results</p>\r\n\r\n<p><strong>Seed </strong>: 42</p>\r\n\r\n<p>Testing Accuracy : 0.8789</p>\r\n\r\n<p>Testing Fbeta : 0.8748</p>\r\n\r\n<p>Testing Recall : 0.9283</p>\r\n\r\n<p>Testing Precision : 0.8045</p>\r\n\r\n<p><strong>Seed </strong>:789</p>\r\n\r\n<p>Testing Accuracy : 0.8775</p>\r\n\r\n<p>Testing Fbeta : 0.8739</p>\r\n\r\n<p>Testing Recall : 0.9311</p>\r\n\r\n<p>Testing Precision : 0.8022</p>\r\n\r\n<p><strong>Scores are similar for different random seeds. For the 4 scores, after testing on 2 models</strong></p>\r\n\r\n<p>&nbsp;</p>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(10,'Machine Learning Algorithm to Predict Used Cars Prices in Egypt','filter-python,filter-statistic,filter-machine_learning','portfolio_photos/Used_Cars.png','Developing a machine learning algorithm to predict used car prices based on historical data and various features.',replace(replace('<ul>\r\n	<li><strong>Data Exploration:</strong>\r\n	<ul>\r\n		<li>Imported libraries like Pandas, NumPy, Matplotlib, Seaborn, StatsModels.</li>\r\n		<li>Read and explored the &#39;cars.csv&#39; dataset, examining column values and addressing issues like kilometer data.</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Data Preprocessing:</strong>\r\n	<ul>\r\n		<li>Engineered &#39;KM_Adj&#39; feature from the &#39;Kilometers&#39; column.</li>\r\n		<li>Processed and manipulated data for analysis by dropping irrelevant columns and handling categorical variables.</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Exploratory Data Analysis (EDA):</strong>\r\n	<ul>\r\n		<li>Explored relationships between &#39;Price&#39; and &#39;Year&#39; using scatter plots.</li>\r\n		<li>Conducted OLS (Ordinary Least Squares) regression to analyze relationships and assess model fit.</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Testing and Model Development:</strong>\r\n	<ul>\r\n		<li>Tested OLS assumptions and assessed multicollinearity.</li>\r\n		<li>Performed feature selection using F-statistics and P-values.</li>\r\n		<li>Standardized features and applied Linear Regression for modeling used car prices.</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Model Evaluation:</strong>\r\n	<ul>\r\n		<li>Evaluated multicollinearity using variance inflation factor (VIF).</li>\r\n		<li>Utilized transformed data and employed Linear Regression for predicting prices.</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Further Steps:</strong>\r\n	<ul>\r\n		<li>Extracted target and input variables for the model.</li>\r\n		<li>Utilized StandardScaler for preprocessing inputs and conducted model testing.</li>\r\n	</ul>\r\n	</li>\r\n</ul>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_wtnQgsc.png" style="height:150px; width:272px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_HITP0M0.png" style="height:510px; width:722px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_q4A6AU2.png" style="height:365px; width:633px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_MgHLE4n.png" style="height:410px; width:445px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_V3uOMeV.png" style="height:742px; width:726px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_afywqov.png" style="height:783px; width:718px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_AIwmujo.png" style="height:848px; width:543px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_fsC33E1.png" style="height:775px; width:661px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_GuB6mPH.png" style="height:440px; width:513px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_By1rsjP.png" style="height:680px; width:633px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_j9lWeDc.png" style="height:296px; width:600px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_hqWvTMq.png" style="height:841px; width:594px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_tGGx6lj.png" style="height:251px; width:746px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_IHZu0Ko.png" style="height:405px; width:858px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_quin57H.png" style="height:780px; width:772px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_CNwujx9.png" style="height:708px; width:867px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_8DC446t.png" style="height:299px; width:468px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_M4yJLm6.png" style="height:606px; width:547px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_dq0XpnC.png" style="height:688px; width:835px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_vDFvtwp.png" style="height:316px; width:435px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_Nw2tQlo.png" style="height:241px; width:563px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_3eXroOb.png" style="height:264px; width:519px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_tXkGEVj.png" style="height:494px; width:575px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_VZUp1Cn.png" style="height:486px; width:853px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_YsUc6Vx.png" style="height:654px; width:824px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_FLNB65h.png" style="height:383px; width:640px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_xRzZzf6.png" style="height:380px; width:827px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_gnoCtDh.png" style="height:330px; width:849px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_ufmz8AH.png" style="height:386px; width:829px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_fdxASME.png" style="height:733px; width:820px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_5scKE3c.png" style="height:637px; width:639px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_wff5NTt.png" style="height:362px; width:845px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_FjpO3lK.png" style="height:519px; width:827px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_SSGqa0E.png" style="height:631px; width:672px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_GkTulfE.png" style="height:588px; width:824px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_eNPmuOl.png" style="height:390px; width:524px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(11,'Deploying and Monitoring Image Classification Workflow Using Amazon AWS','filter-python,filter-machine_learning','portfolio_photos/ML_Sconed_4.jpg','Deploying an Image Classification workflow involves ETL operations to ready the CIFAR dataset, Lambda function development, and Step Functions orchestration. Testing and evaluation using SageMaker Model Monitor ensure reliable predictions and error handling.',replace(replace('<p>&nbsp;</p>\r\n\r\n<p><strong>Setting Up the Notebook:</strong></p>\r\n\r\n<ul>\r\n	<li>Utilized Python 3 (Data Science) kernel on an ml.t3.medium SageMaker notebook instance.</li>\r\n	<li>Prepared the environment for working with the CIFAR dataset for Image Classification.</li>\r\n</ul>\r\n\r\n<p><strong>Data Staging:</strong></p>\r\n\r\n<ul>\r\n	<li>Extracted the CIFAR-100 dataset from the University of Toronto&#39;s hosting service.</li>\r\n	<li>Performed ETL (Extract, Transform, Load) operations to prepare the data into a usable format for production.</li>\r\n</ul>\r\n\r\n<p><strong>Drafting Lambdas and Step Function Workflow:</strong></p>\r\n\r\n<ul>\r\n	<li>Developed and deployed three Lambda functions to work with a simplified data object.</li>\r\n	<li>Organized these Lambda functions using the Step Functions visual editor, chaining them together to create a workflow.</li>\r\n	<li>Configured inputs and outputs for Lambda functions within the Step Functions, ensuring proper handling and descriptive naming.</li>\r\n</ul>\r\n\r\n<p><strong>Testing and Evaluation:</strong></p>\r\n\r\n<ul>\r\n	<li>Executed multiple Step Function invocations using data from the provided test folder.</li>\r\n	<li>Ensured the workflow correctly passed predictions to downstream systems while appropriately handling errors.</li>\r\n	<li>Utilized SageMaker Model Monitor to generate recordings of data and inferences for visualization.</li>\r\n	<li>Conducted test inputs generation using a function to verify workflow performance.</li>\r\n</ul>\r\n\r\n<p><strong>Conclusion:</strong> The workflow successfully underwent deployment and testing, reliably passing predictions and handling errors as expected. Through Step Functions orchestration and SageMaker Model Monitor utilization, the machine learning workflow for Image Classification exhibited consistent and reliable behavior, vital for its successful deployment in production settings.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_3erGbu5.png" style="height:85px; width:372px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_MtpJe3d.png" style="height:190px; width:388px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_NiizflF.png" style="height:640px; width:436px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_XQSPljz.png" style="height:488px; width:476px" /></p>\r\n\r\n<h2>Model Training</h2>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_O2NQxNt.png" style="height:676px; width:557px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_IgFUskx.png" style="height:358px; width:428px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_hB5yv0J.png" style="height:464px; width:933px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_IeoUPjo.png" style="height:735px; width:943px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_dGOmASP.png" style="height:714px; width:871px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_ckgW839.png" style="height:774px; width:764px" /></p>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/30/ml-sconed-2.jpg" style="height:472px; width:800px" /><img alt="" src="/media/content/ckeditor/2023/12/30/ml-sconed-3.jpg" style="height:569px; width:800px" /><img alt="" src="/media/content/ckeditor/2023/12/30/ml-sconed-4.jpg" style="height:323px; width:800px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(12,'Scraping TCG Player Prices for Optimal Deals','filter-python,filter-machine_learning,filter-scrapping','portfolio_photos/tcgplayer_cover.png','Scraping online store of TCG Player to compare sales price against market rates, facilitating the identification of the best deals.',replace(replace('<p>&nbsp;</p>\r\n\r\n<p><strong>Workflow:</strong></p>\r\n\r\n<ol>\r\n	<li>\r\n	<p><strong>Data Collection:</strong></p>\r\n\r\n	<ul>\r\n		<li>Develop web scraping tools to extract card game prices from TCG Player.</li>\r\n		<li>Gather comprehensive data encompassing card details, store prices, and additional attributes.</li>\r\n	</ul>\r\n	</li>\r\n	<li>\r\n	<p><strong>Data Comparison and Analysis:</strong></p>\r\n\r\n	<ul>\r\n		<li>Compare scraped prices against market rates to identify discrepancies and advantageous deals.</li>\r\n		<li>Analyze price trends, fluctuations, and variations across stores and card types.</li>\r\n	</ul>\r\n	</li>\r\n	<li>\r\n	<p><strong>Optimization:</strong></p>\r\n\r\n	<ul>\r\n		<li>Implement algorithms to identify optimal deals based on scraped data and market prices.</li>\r\n		<li>Formulate strategies to recommend the best deals to users, considering factors like card condition, rarity, and price differences.</li>\r\n	</ul>\r\n	</li>\r\n	<li>\r\n	<p><strong>Visualization and Reporting:</strong></p>\r\n\r\n	<ul>\r\n		<li>Visualize price variations, trends, and comparative analyses through charts, graphs, or dashboards.</li>\r\n		<li>Generate reports showcasing the best deals and price differentials for user reference.</li>\r\n	</ul>\r\n	</li>\r\n</ol>\r\n\r\n<p><strong>Insight:</strong>&nbsp;Utilizing web scraping techniques, this project aims to amass card game prices from diverse online stores, enabling a comprehensive comparison with market rates. Through analysis and optimization algorithms, the goal is to unearth the most favorable deals for users, factoring in card attributes and price differentials. Ultimately, this initiative seeks to empower users with actionable insights, allowing them to make informed purchasing decisions for card games.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_kbdknHP.png" style="height:470px; width:894px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_yHl9qWx.png" style="height:503px; width:867px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_T7UvSQq.png" style="height:611px; width:886px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_ENRnbMR.png" style="height:381px; width:874px" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(13,'SQL Analysis of online Music store Database','filter-dashboard,filter-excel,filter-sql','portfolio_photos/SQL.PNG','Analyzing a music Store database using SQL to offer insights into popular genres, countries, and song attributes. Leveraging SQL queries, this analysis aims to uncover user preferences, top genres, and average song lengths.',replace(replace('<ul>\r\n	<li><strong>Objective:</strong> Conduct an in-depth analysis of a music cloud platform&#39;s database using SQL queries to extract valuable insights.</li>\r\n	<li><strong>Queries:</strong>\r\n	<ul>\r\n		<li>Which Genre sells the most</li>\r\n		<li>Which Country buys more</li>\r\n		<li>Which artist appears the most in playlist</li>\r\n	</ul>\r\n	</li>\r\n	<li><strong>Visualization:</strong>\r\n	<ul>\r\n		<li>Utilize Excelto create visual representations of the SQL query results.</li>\r\n		<li>Generate bar charts illustrating top genres listened to and average song length by genre.</li>\r\n	</ul>\r\n	</li>\r\n</ul>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_KSqRvEN.png" style="height:450px; width:800px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_NijYseC.png" style="height:449px; width:800px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_m4klvvQ.png" style="height:453px; width:800px" /></p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image_994xZ2j.png" style="height:449px; width:800px" /></p>','\r',char(13)),'\n',char(10)));
INSERT INTO homesite_portfolio VALUES(14,'Exploratory Data Analysis of US Census Demographic Data','filter-dashboard,filter-sql,filter-tableau_BI','portfolio_photos/Tablaeu_1.PNG','Exploring US Census Demographic Data, this analysis uses varied visualizations to reveal correlations between poverty, employment, and their impact on income and unemployment, aiding in understanding complex demographic trends and informing policy decisions.',replace(replace('<p>&nbsp;</p>\r\n\r\n<ul>\r\n	<li><strong>Objective:</strong> Explore and derive insights from US Census Demographic Data.</li>\r\n	<li><strong>Visualization Techniques:</strong>\r\n	<ul>\r\n		<li><strong>Map Chart:</strong> Represents population distribution across states with a color scale for a comprehensive overview.</li>\r\n		<li><strong>Bar Charts:</strong> Depict poverty and child poverty, showcasing correlations between these factors across states.</li>\r\n		<li><strong>Pie Charts:</strong> Illustrate work fields and employment types, providing percentage breakdowns within each category.</li>\r\n		<li><strong>Scatter Plot:</strong> Examines relationships between poverty and income, and poverty and unemployment, revealing correlations between numerical factors.</li>\r\n	</ul>\r\n	</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/30/tablaeu-1.PNG" style="border-style:solid; border-width:2px; float:left; height:441px; margin:10px; width:1000px" /><img alt="" src="/media/content/ckeditor/2023/12/30/tablaeu-2.PNG" style="border-style:solid; border-width:2px; float:left; height:446px; margin:10px; width:1000px" /></p>\r\n\r\n<p><img alt="" src="/media/content/ckeditor/2023/12/30/tablaeu-3.png" style="height:444px; width:1000px" /><img alt="" src="/media/content/ckeditor/2023/12/30/tablaeu-4.png" style="height:383px; width:1000px" /><img alt="" src="/media/content/ckeditor/2023/12/30/tablaeu-5.png" style="height:339px; width:1000px" /><img alt="" src="/media/content/ckeditor/2023/12/30/tablaeu-6.png" style="height:366px; width:1000px" /></p>','\r',char(13)),'\n',char(10)));
CREATE TABLE IF NOT EXISTS "homesite_blogposts" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "title" varchar(100) NOT NULL, "category" varchar(100) NOT NULL, "insight" varchar(259) NOT NULL, "body" text NOT NULL, "photo" varchar(100) NULL, "subscribed_at" datetime NOT NULL);
INSERT INTO homesite_blogposts VALUES(1,'اية الفرق بين ال المتوسط , الوسط , المنوال','filter-barabic,filter-bdata,filter-bstatistic','اية الفرق بين ال mean, mode, median',replace(replace('<p>من الحاجات اللي بتسمعها كتير في تحليل البيانات هي ال</p>\r\n\r\n<p><strong>Mean</strong>,&nbsp;<strong>Median</strong>,&nbsp;<strong>Mode</strong></p>\r\n\r\n<p>اللي ترجمتهم بالعربي بنفس الترتيب&nbsp;المتوسط , الوسط , المنوال</p>\r\n\r\n<p>اول حاجة عايزين نشرح اية هما الحاجات دي بسرعة و بعدين ايه سبب وجودهم, و بنستخدمهم في اية</p>\r\n\r\n<p>اولا ال&nbsp;<strong>mean&nbsp;</strong>و دة عبارة عن متوسط البيانات يعني مثلا لو عندنا اعمار الموطفين في ادارة معينه و كانت الاعمار بالشكل دة</p>\r\n\r\n<p>31,33,33,35,37,41,42</p>\r\n\r\n<p>اللي بنعمله ببساطة اننا بنجمع الارقام دي و نقسم على عدد الناس</p>\r\n\r\n<p>(42+31+33+33+35+37+41)/7 = 36</p>\r\n\r\n<p>دة ال mean&nbsp;او المتوسط بتاع الاعمار</p>\r\n\r\n<p>تاني حاجة هي ال&nbsp;<strong>median&nbsp;</strong>او الوسيط و دة بيشوف اي الرقم اللي في الوسط بالنسبة للبيانات دي</p>\r\n\r\n<p>في نفس المثل اللي فات هما 7 اعمار و الرقم اللي في الوسط هو 35</p>\r\n\r\n<p>دة االوسط بتاع الاعمار</p>\r\n\r\n<p>اخر حاجة هو ال المنوال و هو اكتر رقم اتكرر في المجموعة و في الحالة دي هو ال 33&nbsp;</p>\r\n\r\n<p>طبعاً جوة كل نوع في تفاصيل كتير, في داتا ملهاش منوال و في كذا نوع للمتوسط بس احنا هنتكلم عن الفكرة العامة بس دلوقتي&nbsp;&nbsp;</p>\r\n\r\n<p>طيب احنا كدة بالنسبة للبيانات الموجودة في 3 ارقام ينفعوا متوسط 33 و 35 و 36</p>\r\n\r\n<p>في سؤالين مهمين دلوقتي, لية كل دة ما حاجة منهم كفاية و خلاص</p>\r\n\r\n<p>تاني سؤال هو&nbsp;&nbsp;طالما عندنا 3 اختيارات للمتوسط المفروض نختار انهي منهم؟</p>\r\n\r\n<p>السؤالين اجابتهم تبان في شوية مواقف عشان نعرف نميز بينهم</p>\r\n\r\n<p>اول حاجة افترض انك عايز تعرف متوسط المرتبات في شركة معينه</p>\r\n\r\n<p>المرتبات دي هتفرق جداً بينهم&nbsp;بين بعض, يعني صعب جداً تلاقي 2 بنفس المرتب بالظبط, في زيادات سنوية و خصومات , فصعب جداً تلاقي المنوال. كدة ناقص انك تشوف المتوسط و الوسط</p>\r\n\r\n<p>المتوسط شكله منطقي جداً, هنجمع كل المرتبات و نقسم على عدد الموظفين, بس خلي بالك من نقطة مهمة جدا</p>\r\n\r\n<p>شركة زي كبيرة عندها 1000 موظف , ممكن يبقى المتوسط الحقيقي لمرتب الموظفين فيها هو 5 الاف جنية</p>\r\n\r\n<p>بس دة من غير مدير الشركة و ال C&nbsp;ليفيل , ال 10 مديرين دول مرتبهم لوحدهم ممكن يوصل ل 5 مليون جنية او اكتر</p>\r\n\r\n<p>لما تحسبهم و تشوف المتوسط هتلاقي ان متوسط المرتب طلع من 5 الاف ل 10 الاف جنية, بس دة مش حقيقي , الرقم الكبير دة اسمه&nbsp;<strong>outlier&nbsp;</strong>لما يبقي في رقم بعيد اوي عن متوسط الارقام التانية, دة ممكن يبوظلك حسابك لل&nbsp;<strong>mean</strong></p>\r\n\r\n<p>&nbsp;في حالة زي دي الافضل انك تستخدم ال&nbsp;<strong>median&nbsp;</strong>, تشوف الشخص اللي في الوسط بياخد كام .</p>\r\n\r\n<p>مثل تاني, لو انت هتعمل مصنع بنطلونات و عايز تعرف البنطلون فيه كام رجل</p>\r\n\r\n<p>ف جمعت بيانات عن عدد رجول الناس , ف قررت انك هاتعمل البنطلونات كل بنطلون فيه 1.99 رجل , يعني في رجل اقصر شوية لان في ناس معندهاش رجلين , فلما جمعت الارقام و حسبت ال&nbsp;<strong>mean&nbsp;</strong>طلع اقل من 2 , هنا طبعاً هتقول لأ , انا ممكن استخدم ال&nbsp;<strong>median&nbsp;</strong>زي المثل اللي فات</p>\r\n\r\n<p>هتلاقي ان في ناس عندها رجلين , و ناس برجل واحدة , و ناس معندهاش رجلين خالص, ف ال&nbsp;<strong>median&nbsp;</strong>هنا بيقول اعمل بنطلون برجل واحدة, طبعاً دة مش منطقي برضة!</p>\r\n\r\n<p>الاختيار الصح هو ال&nbsp;<strong>mode&nbsp;</strong>اكتر رقم اتكرر عندك في عدد الرجول هو 2 , اكتر من 99.99% من الناس عندها رجلين, يبقى دة المنطقي استخدامه</p>\r\n\r\n<p>الخلاصة هو ان في اكتر من وسيله عشان تعرف متوسط البيانات, و دة مفيد ليك جداً في قرارات كتير في الشغل, و اختيارك لانهي طريقة الاصح دة حسب الموقف و حسب انتي عايز تاخد انهي قرار في الداتا, حد عندة امثله تاني لقرارات ممكن تبقى صعبة في اختيار المتوسط؟</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt="No alt text provided for this image" src="https://media.licdn.com/dms/image/C4E12AQGcCM8mlJAaNQ/article-inline_image-shrink_400_744/0/1604065778329?e=1709164800&amp;v=beta&amp;t=DAdizsmP1NXeskdRC6cFC97NSe-aauVm89WUTwifz3s" style="height:413px; width:600px" /></p>','\r',char(13)),'\n',char(10)),'portfolio_photos/mode.png','2023-12-30 13:59:07.382236');
INSERT INTO homesite_blogposts VALUES(2,'Difference between Mode, Mean, Median','filter-benglish,filter-bdata,filter-bstatistic','What is the Difference between Mode, Mean, Median',replace(replace('<p>One of the fundamental concepts in data analysis is&nbsp;<strong>Mean</strong>,&nbsp;<strong>Median</strong>, and&nbsp;<strong>Mode</strong>.</p>\r\n\r\n<p>Firstly we need to describe the differences between those 3 concepts and how can use them.</p>\r\n\r\n<p><strong>Mean&nbsp;</strong>is the average of the data set; let&rsquo;s assume that we want to calculate the&nbsp;<strong>mean&nbsp;</strong>of employees&rsquo; ages in a specific department and we found the data as below</p>\r\n\r\n<p>31,33,33,35,37,41,42&nbsp;</p>\r\n\r\n<p>Simply you add those numbers together then divide by the number of employees</p>\r\n\r\n<p>(42+31+33+33+35+37+41)/7 = 36</p>\r\n\r\n<p>This is the&nbsp;<strong>mean&nbsp;</strong>of employees&rsquo; ages&nbsp;</p>\r\n\r\n<p><strong>Median&nbsp;</strong>is the middle number of the data set when we sort numbers in ascending order, in the same example we will find that middle age is 35&nbsp;</p>\r\n\r\n<p>Finally the&nbsp;<strong>mode</strong>,&nbsp;<strong>Mode&nbsp;</strong>is the most common number in a data set, in the same example, the most repeated number in this data set is 33&nbsp;</p>\r\n\r\n<p>Of course, there are more details in those concepts, you can find data set without&nbsp;<strong>mode</strong>, you can calculate many types of&nbsp;<strong>mean</strong>, but let&rsquo;s focus now on the main concept.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Now as we analyzed the data, we found 3 outputs, 33, 35, and 36.</p>\r\n\r\n<p>Now there are 2 important questions, why the confusion between all of these concepts, isn&rsquo;t one of them is enough?</p>\r\n\r\n<p>The second question is which one we should be using.</p>\r\n\r\n<p>Let&rsquo;s answer with a few examples,</p>\r\n\r\n<p>Let&rsquo;s assume that we want to know the average salaries in a specific company</p>\r\n\r\n<p>When you think about it, there will always be a variance&nbsp;between salaries because of salary increase, or social insurance ranges, etc., so it won&rsquo;t be effective to find the&nbsp;<strong>mode&nbsp;</strong>of the data.</p>\r\n\r\n<p>The&nbsp;<strong>mean&nbsp;</strong>seems to be more appealing, summing all salaries and dividing by the number of employees, but you should take something important into consideration</p>\r\n\r\n<p>For a company with 1000 employees, the actual&nbsp;<strong>mean&nbsp;</strong>might be around 5K EGP for example, but when you consider the CEO and other C level management which salaries might reach 5 Million EGP, the average will jump from 5K EGP to 10K EGP, which is not true at all.</p>\r\n\r\n<p>In this case, it&rsquo;s preferable to use the&nbsp;<strong>Median</strong>; it will give you the most accurate estimation for the average.</p>\r\n\r\n<p>Another example, if you are about to start a factory to produce pants, now you need to know the number of legs in each one ( I know it looks absurd, but let&rsquo;s check what the numbers will propose)</p>\r\n\r\n<p>First, you collected the data of a sample for the number of legs for each one in this data set, then you took your decision, you will go with the&nbsp;<strong>mean</strong>, and produce new pants each one has 1.99 legs.</p>\r\n\r\n<p>This doesn&rsquo;t seem like a wise decision, so you decided to go with the&nbsp;<strong>median&nbsp;</strong>like the previous example.</p>\r\n\r\n<p>By collecting the data, some people have 2 legs, others have 1 leg, and finally, there are people with Zero legs, so the&nbsp;<strong>median&nbsp;</strong>is 1, you will produce pants, each with 1 leg,</p>\r\n\r\n<p>Still not a wise decision, finally you will find that the best decision is going with the&nbsp;<strong>mode</strong>; more than 99% of people have 2 legs, so in this example&nbsp;<strong>mode&nbsp;</strong>is our best option.</p>\r\n\r\n<p>To recap, there are many ways to find the average of the data, and it&rsquo;s very useful to use in business decisions, choosing which method is the best is according to the situation and the type of decision you want to take.</p>\r\n\r\n<p>Do you have any examples where it was hard to find the average?</p>\r\n\r\n<p><img alt="No alt text provided for this image" src="https://media.licdn.com/dms/image/C4E12AQGcCM8mlJAaNQ/article-inline_image-shrink_400_744/0/1604065778329?e=1709164800&amp;v=beta&amp;t=DAdizsmP1NXeskdRC6cFC97NSe-aauVm89WUTwifz3s" style="height:413px; width:600px" /></p>','\r',char(13)),'\n',char(10)),'portfolio_photos/mode_MRwwd5r.png','2023-12-30 16:44:24.992774');
INSERT INTO homesite_blogposts VALUES(3,'اية هو الانحراف المعياري؟ ليه كنا بنذاكر الحاجات دي؟','filter-barabic,filter-bdata,filter-bstatistic','اية هو الانحراف المعياري؟ ليه كنا بنذاكر الحاجات دي؟',replace(replace('<p style="direction:rtl">من اول الصدمات اللي اخدناها في المدرسة و احنا صغيرين هي حاجة اسمها الانحراف المعياري</p>\r\n\r\n<p style="direction:rtl">دي تقريباً اول حاجة خلتنا نسأل الاسئله الاهم في حياتنا, هو اية الكلام دة, و انا بذاكره ليه؟</p>\r\n\r\n<p style="direction:rtl">مع الاسف محدش و احنا صغيرين اهتم انه يفهمنا اهمية الكلام دة في حياتنا, ف تعالوا نتكلم عنه من الاول خالص</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">اية هو الانحراف المعياري؟</p>\r\n\r\n<p style="direction:rtl">فاكرين في المقال اللي قبل دة عن المتوسط و الوسط&nbsp; و المنوال</p>\r\n\r\n<p style="direction:rtl">Mean, Median, Mode</p>\r\n\r\n<p style="direction:rtl">المقال من هنا</p>\r\n\r\n<p style="direction:rtl"><a href="https://bit.ly/37UvQsH" style="color:blue; text-decoration:underline">https://bit.ly/37UvQsH</a></p>\r\n\r\n<p style="direction:rtl">لما قلنا اننا بنحاول نعرف معلومات عن البيانات بتاعتنا اول حاجة هي متوسط البيانات دي</p>\r\n\r\n<p style="direction:rtl">خلينا نمشي ب مثل انك عايز تعرف معلومات عن اطوال و اوزان الناس في مجموعة معينه هنسميها &quot;المجموعة&quot;</p>\r\n\r\n<p style="direction:rtl">ف اول حاجة اخدت الاطوال و الاوزان و لقيت ان متوسط الطول هو 165 سم و متوسط الوزن هو 80 كيلو</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">لحد هنا تمام اوي, بس انا هسألك سؤال, هل انت من المعلومة اللي فاتت دي, تقدر تعرف اذا كان المجموعة دي فيها ناس طويلة اوي او قصيرة اوي او لا؟</p>\r\n\r\n<p style="direction:rtl">مش هتعرف طبعاً</p>\r\n\r\n<p style="direction:rtl">يعني مثلا المجموعة ممكن يبقى اطوالها و دي مجموعة 1</p>\r\n\r\n<p style="direction:rtl">145, 145, 150, 160, 180, 180, 180, 180</p>\r\n\r\n<p style="direction:rtl">3 قصيرين جداً و 4 طوال جدأ و واحد متوسط</p>\r\n\r\n<p style="direction:rtl">او تبقى كدة و دي مجموعة 2</p>\r\n\r\n<p style="direction:rtl">170, 165, 165, 165, 165, 165, 165, 160</p>\r\n\r\n<p style="direction:rtl">كلهم تقريبا في نفس المتوسط</p>\r\n\r\n<p style="direction:rtl">هنا بيبان فايدة الانحراف المعياري , اللي بيقولنا ببساطة هو في مجموعة البيانات دي تقريباً كدة الناس بتبعد اد اية عن المتوسط</p>\r\n\r\n<p style="direction:rtl">مجموعة واحد مثلا الانحراف المعياري بتاعها 15.6 &nbsp;و المجموعة التانية الانحراف المعياري بتاعها 2.5</p>\r\n\r\n<p style="direction:rtl">واخد بالك الفرق كبير ازاي؟</p>\r\n\r\n<p style="direction:rtl">عشان معظم الناس اللي في مجموعة واحد بعيد طولهم بعيد عن المتوسط, اما تاني مجموعة ف معظمهم قريب للمتوسط</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">لو عايز الموضوع بطريقة رياضية &nbsp;, فنقدر نقول ان على بعد 1 انحراف معياري من المتوسط (يمين و شمال) هتلاقي 68% تقريباً من الناس</p>\r\n\r\n<p style="direction:rtl">يعني تاني مجموعة المتوسط كان 165 سم عشان نعرف المجوعة اللي فيها 68% من الناس دي, مرة هنزود و مرة هننقص الانحراف المعياري</p>\r\n\r\n<p style="direction:rtl">ف 165 &ndash; 2.5 = 162.5</p>\r\n\r\n<p style="direction:rtl">و 165 + 2.5 = 167.5</p>\r\n\r\n<p style="direction:rtl">يعني حوالي 68% من الناس طولهم بين 162.5 و 167.5 و لما تحسب هتلاقي ان اللي في المثل 75% من الناس فعلاً في الاطوال دي</p>\r\n\r\n<p style="direction:rtl">لما تجرب على اول مجموعة هيبقى نفس الكلام</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">طريقة حساب الاتحراف المعياري ممكن نتكلم عنها بعد كدة و هي سهله جداً عموماً</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">طبعاً استخداماته مهمه جداً ف كل حاجة, زي المصانع مثلاً لانهم بيقيسوا بيه نسبة الخطاً في الانتاج</p>\r\n\r\n<p style="direction:rtl">الانحراف المعياري بيتقال عليه سيجما, لو حد سمع عن ال Six Sigma</p>\r\n\r\n<p style="direction:rtl">ف دة بيحاول يوصل ان اياً كان المنتج او الخدمة بتاعتك يبقى نسبة الخطاً فيها six sigma</p>\r\n\r\n<p style="direction:rtl">يعني اية؟ اقولك</p>\r\n\r\n<p style="direction:rtl">مش قلنا ان الانحراف المعياري الواحد جواة 68% من البيانات</p>\r\n\r\n<p style="direction:rtl">يعني لو شغالين في الانتاج و شغالين ب 1 sigma</p>\r\n\r\n<p style="direction:rtl">32% من المنتجات هتبقى بايظة , الجدول دة بيقول كل انحراف معياري زيادة بيزود النسبة اد اية</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl"><img src="/media/content/ckeditor/2023/12/30/image-20231230204846-1.jpeg" style="height:393px; width:632px" /></p>\r\n\r\n<p style="direction:rtl">زي ما باين كدة نسبة الغلط لو شغال على 6 انحراف معياري هي 3.4 في المليون و دي نسبة اكتر من رائعة</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">طب احنا قاعدين نتكلم برضة عن تفاصيل كتير في الشغل و التصنيع, برضة انا كشخص في حياتي ممكن استفيد منه اية ؟</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">اقولك كذا مثل بسيط</p>\r\n\r\n<p style="direction:rtl">لو انت من اللي بيتابعوا اسعار الاسهم مثلا و عايز تشتري و شايف ان السهم سعرة كويس و قررت تشتري, لازم تشوف الانحراف المعياري بتاعه, كل ما الانحراف المعياري بتاعه يزيد يبقى دة سهم مش مستقر و ميفضلش تشتري فيه الا لو عارف كويس انت عايز تشتريه ليه</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">هتشتري شقة و شفت متوسط اسعار الشقق اللي نفس المساحة في المنطقة, جميل, بس احسب الانحراف المعياري عشان تعرف هي الاسعار مستقرة في المكان ولا لأ</p>\r\n\r\n<p style="direction:rtl">انحراف معياري قليل يبقي المنطقة تقريبا كلها مستواها واحد و مفيش مشاكل في حته و حته لا</p>\r\n\r\n<p style="direction:rtl">انحراف معياري كبير يعني في حاجة غريبة, لية شقتين في نفس المنطقة بنفس المساحة في فروق كبيرة في الاسعار بينهم, دة معناه ان في مشكله ممكن تحتاج تدور عليها, و هكذا</p>\r\n\r\n<p style="direction:rtl">&nbsp;</p>\r\n\r\n<p style="direction:rtl">حد عندة مثل تاني&nbsp; ممكن تستخدم فيه الانحراف المعياري بعيد عن الشغل؟</p>\r\n\r\n<p>&nbsp;</p>','\r',char(13)),'\n',char(10)),'portfolio_photos/stand.jpg','2023-12-30 16:49:16.666504');
INSERT INTO homesite_blogposts VALUES(4,'What is the “Standard Deviation?”','filter-benglish,filter-bdata,filter-bstatistic','What is the “Standard Deviation?”',replace(replace('<p>By definition Standard Deviation &ldquo;SD&rdquo; is the measure of the amount of variation or dispersion of a set of values.</p>\r\n\r\n<p>In a simple way, when you observe any set of data, let us say the height 10 people</p>\r\n\r\n<p>The height mean is 165 CM, what is the amount of variation in their heights?</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Everything is possible, from all having the same height or all are extremely short and tall, adding up to give the same mean.</p>\r\n\r\n<p>How can you find out about this info with only one number, without looking into the whole data set?</p>\r\n\r\n<p>The answer is the SD</p>\r\n\r\n<p>In the SD is zero or too small, that means there is almost no variation in our data, in this example, if the SD is zero, then all of them are 165 CM, the more the SD, the more there is difference in their heights</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Note:</p>\r\n\r\n<p>You can add 1 SD to the mean, and subtract 1 SD from the mean; this range will contain 68% from the whole data set, which gives you an idea of your data distribution.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>In real life</p>\r\n\r\n<p>If you want to invest in stock, and its price seems good, before you decide to buy, you must check the SD. If the SD is high, this means the stock is price not stable and it&rsquo;s not recommended to invest.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>You will buy an apartment and see the average price of apartments of the same area in the area, nice, but calculate the standard deviation so that you know whether the prices are stable in the place or not</p>\r\n\r\n<p>A small standard deviation keeps almost the entire area at the same level, and there are no problems with it and even no</p>\r\n\r\n<p>A large standard deviation means in a strange need, there are two apartments in the same area with the same area in large differences in prices between them, this means that in a problem you may need to revolve on it, and so on</p>\r\n\r\n<p><img src="/media/content/ckeditor/2023/12/30/image-20231230205003-1.jpeg" style="height:393px; width:632px" /></p>\r\n\r\n<p>Where else would you use the standard deviation out of work?</p>\r\n\r\n<p>&nbsp;</p>','\r',char(13)),'\n',char(10)),'portfolio_photos/stand_gcbNpF5.jpg','2023-12-30 16:50:15.034540');
DELETE FROM sqlite_sequence;
INSERT INTO sqlite_sequence VALUES('django_migrations',35);
INSERT INTO sqlite_sequence VALUES('django_admin_log',79);
INSERT INTO sqlite_sequence VALUES('django_content_type',12);
INSERT INTO sqlite_sequence VALUES('auth_permission',48);
INSERT INTO sqlite_sequence VALUES('auth_group',0);
INSERT INTO sqlite_sequence VALUES('auth_user',1);
INSERT INTO sqlite_sequence VALUES('homesite_subscribetonewsletter',7);
INSERT INTO sqlite_sequence VALUES('homesite_contact',17);
INSERT INTO sqlite_sequence VALUES('homesite_portfolio',14);
INSERT INTO sqlite_sequence VALUES('homesite_blogposts',4);
CREATE UNIQUE INDEX "auth_group_permissions_group_id_permission_id_0cd325b0_uniq" ON "auth_group_permissions" ("group_id", "permission_id");
CREATE INDEX "auth_group_permissions_group_id_b120cbf9" ON "auth_group_permissions" ("group_id");
CREATE INDEX "auth_group_permissions_permission_id_84c5c92e" ON "auth_group_permissions" ("permission_id");
CREATE UNIQUE INDEX "auth_user_groups_user_id_group_id_94350c0c_uniq" ON "auth_user_groups" ("user_id", "group_id");
CREATE INDEX "auth_user_groups_user_id_6a12ed8b" ON "auth_user_groups" ("user_id");
CREATE INDEX "auth_user_groups_group_id_97559544" ON "auth_user_groups" ("group_id");
CREATE UNIQUE INDEX "auth_user_user_permissions_user_id_permission_id_14a6b632_uniq" ON "auth_user_user_permissions" ("user_id", "permission_id");
CREATE INDEX "auth_user_user_permissions_user_id_a95ead1b" ON "auth_user_user_permissions" ("user_id");
CREATE INDEX "auth_user_user_permissions_permission_id_1fbb5f2c" ON "auth_user_user_permissions" ("permission_id");
CREATE INDEX "django_admin_log_content_type_id_c4bce8eb" ON "django_admin_log" ("content_type_id");
CREATE INDEX "django_admin_log_user_id_c564eba6" ON "django_admin_log" ("user_id");
CREATE UNIQUE INDEX "django_content_type_app_label_model_76bd3d3b_uniq" ON "django_content_type" ("app_label", "model");
CREATE UNIQUE INDEX "auth_permission_content_type_id_codename_01ab375a_uniq" ON "auth_permission" ("content_type_id", "codename");
CREATE INDEX "auth_permission_content_type_id_2f476e4b" ON "auth_permission" ("content_type_id");
CREATE INDEX "django_session_expire_date_a5c62663" ON "django_session" ("expire_date");
COMMIT;
